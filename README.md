<h3><strong><center>SHERLOCK: A Study in Visual Abductive Reasoning</center></strong></h3>

## TL;DR

We collected a large corpus of *abductive inferences* over
images. Abductive reasoning is the act of reasoning about plausible
inferences in the case of uncertianty. Our corpus consists of 383K
inferences across 108K images. Each inference is grounded in images via a bounding box.
Our model predicts an abductive inference given an image and a bounding box. Example
predictions of our best performing model, generated by the scripts in this repo,
are given here:

<p align="center">
  <img src="example_predictions/example1.png" width=350px>
</p>

<p align="center">
  <img src="example_predictions/example2.png" width=350px>
</p>

<p align="center">
  <img src="example_predictions/example3.png" width=350px>
</p>

A full, true random sample of predictions is given
[here.](https://jmhessel.com/projects/sherlock/darpa_self_eval_examples.html),
and is also available in tsv/html format in this repo under `outputs_human_readable/`.


The model is quantiatively evaluated across two sets of metrics.


## How to run the code?

There are 5 scripts, which can all be run by executing `./run_all.sh'. They do the following:

1. Download the model/data.
2. Run the prediction script which runs the model on the data (a GPU really helps speed!)
3. Run automatic evaluations on the predictions.
4. Run human correlation results.
5. Run exploration script that outputs human readable predictions